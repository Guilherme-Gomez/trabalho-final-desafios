---
title: "Desafios e Requisitos dos Projetos Analiticos"
authores: "Davi Almeida Ferreira e Guilherme Rocha Gomez"
date: "Julho de 2021"
output:
  html_document:
        code_folding: hide
        number_sections: no
        toc: yes
        toc_float:
            collapsed: yes
            smooth_scroll: yes
---
<left>

![](simbolo_fgv.png)

Turma: **FGV TBABD-8**  
Professor: **Rafael Lychowski**  
Autores: **Davi Almeida Ferreira** e **Guilherme Rocha Gomez** 


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 10, warning = F, error = F)
```

### Metodologia utilizada

Utilizaremos para a realização deste trabalho a metodologia CRISP-DM

<center>

![](crisp-dm.png)

### Entendimento do Negócio

A Vale pretende prever o desgaste de rodas de vagões e fornecer uma visão futura da frota de rodeiros, para melhor planejamento de menutanção e compra de componentes. 

**Problema:** Prever se o friso da roda estaá abaixo de 26 mm na próxima leitura.

1. A área não tem visibilidade do que vai contecer com o ativos, antecipando as trocas;
2. A cada troca o rodeiro é usinado, e sua vida útil reduzida;
3. Incapacidade de visuzalização de desgastes acelerados em toda a frota de vagões
4. Muitos vagões apresentam reinciência de trocas, e os fatores não são bem claros;

* Base de dados: EFC_WAYSIDES.csv
* Variável dependente: ESPESSURA_FRISO_RODA
* Método: Supervisionado
* Submétodo: Classificação
* Objetivo: Identificar quais os frisos terão menos que 26 cm de friso no próximo ciclo.

### Entendimento dos Dados

```{r include=FALSE}
# Importar bibliotecas que serão utilizadas no trabalho

library(caret)
library(mlbench)
library(dplyr)
library(tidyverse)
library(SparkR)

# Iniciar sessão do SPARK

spark_path <-"C:/spark-3.1.2-bin-hadoop3.2"
if (nchar(Sys.getenv("SPARK_HOME")) < 1) {
  Sys.setenv(SPARK_HOME = spark_path)
}
sparkR.session(master = "local[*]", sparkConfig = list(spark.driver.memory = "6g"))

sqlContext <- sparkR.session(sc)
sparkR.conf()

```

```{r}
# Importar dataset 

pathfile <- "C:/trabalho-final-desafios/EFC_WAYSIDES.csv"

df <- SparkR::read.df(path = pathfile,
                      header='true', 
                      source = "com.databricks.spark.csv", 
                      inferSchema='true')

head(df)

#read.df(sqlContext,)

#persist(df,"DISK_ONLY")
#cache(df) == persist(df,"MEMORY_ONLY")
#cacheTable(sqlContext,"tableNmae")

```

#### Verificação da qualidade dos Dados
##### Verificar a existencia de dados faltantes
##### Verificar se os dados estão com o tipo correto
##### Verificar outliers
##### Eliminar redundancias

#### Identificar as principais estatisticas da Amostra
#### Identificar as principais correlações entre os dados
#### Construir visualizações

```{r}

printSchema(df)

df %>% head()
df %>% nrow()
df_sample <- df %>% sample(withReplacement = F,fraction = 0.0005)


rdf <- collect(df_sample)

collect(df_sample) %>% 
  #mutate(friso_baixo=ifelse(ESPESSURA_FRISO_RODA<26,"Baixo","OK"))%>% 
  ggplot(aes(x=ESPESSURA_FRISO_RODA,y=CAVA_RODA))+
  geom_point()

#Nomeando uma tabela
createOrReplaceTempView(df, "tabela")

#Não será executada a consulta
sdf <- sql("SELECT DATA_HORA_LEITURA  FROM tabela limit 1000")

#Para ter as primeiras linhas
SparkR::head(sdf)

#Para executar toda a consulta
#O resultado será um datafra em R. Ou seja, aqui fazemos uma "conversão".
#collect(sdf)



df_list <- randomSplit(sdf, c(7,3), 2)
df_train <- df_list[[1]]
df_test <- df_list[[2]]

#model <- spark.glm(df_train, ESPESSURA_FRISO_RODA ~., family = "gaussian")




# collect(df)
# 
# DataExplorer::plot_intro(
#   df, 
#   ggtheme = theme_bw(),
#   theme_config = theme(legend.position = "bottom")
# )
# 
# 
# library(visdat)
# df %>% 
# vis_dat( sort_type = TRUE,warn_large_data = FALSE)


df %>% select("CODIGO_RODEIRO") %>% head()



# df_list <- randomSplit(sdf, c(7,3), 2)
# df_train <- df_list[[1]]
# df_test <- df_list[[2]]
# 
# model <- spark.glm(df_train, GENDER ~., family = "gaussian")
# 
# summary(model)
# 
# predictions_1 <- predict(model, df_train)
# local_df <- collect(predictions_1)
# p = local_df$label-local_df$prediction
# mse = mean((p)^2)
# mae = mean(abs(p))
# rmse = sqrt(mse)
# R2 = 1-(sum((p)^2)/sum((local_df$label-mean(local_df$label))^2))
# print("Métricas de avaliação da base de Treino")
# sprintf(" MAE: %.2f", mae)
# sprintf("MSE: %.2f", mse)
# sprintf("RMSE: %.2f", rmse)
# sprintf("R2: %.4f", R2)

```


### Preparação dos Dados

#### Normalizaçao?
#### PCA/Redução de dimensionalidade

### Modelagem

#### Classificação
##### Regressão logística
##### Árvore de decisão
##### Redes neurais
##### Ensemble?

### Avaliação

### Conclusão

